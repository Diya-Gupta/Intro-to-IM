I enjoyed reading this artcile because this was the first time that I had heard of a female who had so much to contribute to the Apollo mission! It speaks a lot about how male-centric the field was and, to a certain extent, still is considering that not a lot of people working in software know about her and her contributions. 
One point in the article that I found really interesting was how everyone, except for Hamilton, assumed that humans are perfect and that there is no need to account for any sort of human error in the code. The fact that these were all super experienced and knowledgable people makes it even more ironic. They put too much confidence in the human brain and just took it as a matter of fact that any errors that would happen would be commited by the machine and that the humans dealing with everything related to the project would never be responsible for an error. Turns out, even though Hamilton included a note to prevent the error for being commited by a human, it still happened and a solution needed to be found at that moment with not a lot of time to think of one. This reminded me of the many times when people start operating an electronic gadget without reading the instructions manual and just assume that they will figure it out as they go. But when the gadget doesn't work because they did something wrong, they don't accept their fault and blame the machine/gadget instead. I have seen this happen multiple times with friends. This also reminded me of Don Norman's approach of making human centric design and affordances. He had mentioned in his book how engineers cannot always understand the design element of their machine and that they need a designer to understand it. An engineer's solution to this problem would be adding code, like Hamilton wanted and a designers solution might have been to change the affordance in some manner so that triggering the wrong programme would just not work. I am not familiar with these things so I can't say for sure but it is interesting to see how a simple assumption that humans know how to interact with machines and that they understand machines enough to not make mistakes could go so wrong.
